---
output: rmarkdown::github_document
---

# Repeat Indeed.com Resumes to JSON for San Francisco
# Heather Geiger - March 21, 2018

## Load libraries.

```{r}
library(stringr)    #For string operations
library(rvest)      #For screen scrapper
library(tokenizers) #
library(tidyverse)  #For Tidyverse
library(RCurl)      #For File Operations
library(dplyr)      #For Manipulating the data frames
library(DT)         #For Data table package
library(curl)

library(RJSONIO)
```

## Step 1 - Get search queries to paste into a browser.

We write a function to search for a given job title in a particular city and state.

Edit: Actually looks like the initial URL builder is now giving results for all keyword matches to data scientist, not just those with job title of data scientist. It was not doing this yesterday. I modify the code to give the correct results today.

```{r}
indeedUrlBuilder <- function(jobtitle, cityname, statecode){
#startUrl <- "https://www.indeed.com/resumes?search=l"
startUrl <- "https://resumes.indeed.com/search?l="
jobtitle <- gsub(" ","%20",jobtitle)
middle0Url <- "%2C%20"
cityname <- gsub(" ","%20",cityname)
middle1Url <- "&q="
endUrl <- "&searchFields=jt"
#searchUrl <- paste0(startUrl,jobtitle,middle0Url,cityname,middle1Url,statecode,endUrl)
searchUrl <- paste0(startUrl,cityname,middle0Url,statecode,middle1Url,jobtitle,endUrl)
return(searchUrl)
}
```

Run for data scientist in San Francisco, CA.

```{r}
url_to_search <- indeedUrlBuilder("data scientist","San Francisco","CA")
url_to_search
```

Pasting this URL into a browser, we find that there are 679 resumes for data scientist in San Francisco, CA.

Let's take the first 650.

```{r}
for(i in seq(from=50,to=600,by=50))
{
print(paste0(url_to_search,"&start=",i))
}
```

## Step 2 - Paste search queries into a browser and use point-and-click to get commands to download JSON files using the API.

Same method as before. This time we save the curl commands in curl_commands_first_650_san_francisco_resumes.txt.

## Step 3 - Run curl commands.

```{r, echo=TRUE,eval=FALSE}
dir.create("data_scientist_san_francisco_CA_search_results_json_files")

curl_commands <- readLines("curl_commands_first_650_san_francisco_resumes.txt")

for(i in 1:length(curl_commands))
{
command <- curl_commands[i]
print(command)
system(paste0(command," > data_scientist_san_francisco_CA_search_results_json_files/resumes_pg",i,".json"))
}
```

```{r, echo=FALSE,eval=TRUE}
curl_commands <- readLines("curl_commands_first_650_san_francisco_resumes.txt")
```

## Step 4 - Process the search result JSON files.

For each JSON file, read into R and use this to get the links to resumes.

```{r, echo=TRUE, eval=TRUE}
resume_ids <- rep(NA,times=length(curl_commands)*50)

for(i in 1:length(curl_commands))
{
json_search_results <- fromJSON(paste0("data_scientist_san_francisco_CA_search_results_json_files/resumes_pg",i,".json"))
j = i - 1
resume_ids[(j*50 + 1):(j*50 + 50)] <- unlist(lapply(json_search_results$results,"[[","accountKey"))
}

head(resume_ids)
tail(resume_ids)
```

## Step 5 - Download JSON files for resumes.

For a given value in resume_ids, we can get the corresponding resume link by pasting to "https://resumes.indeed.com/resume/".

Then, read in the file. Take the line in the HTML file that is being run through the JSON.parse command.

The JSON file will be bounded by curly brackets, then "\\x22" at the beginning. It will end with "\\x22", then curly brackets.

There will also be other "\\x22" occurences in between, so take the first and last one.

Convert "\\x22" to quote, and it should now be in a format where fromJSON will interpret it as a JSON.

Last time, we output each individual JSON to a file.

However, since each JSON we output was one line, it might be better to have each JSON as a string in a character vector.

Then when we are done, output all the resumes as one big file.

This way we can also read in the JSONs from the vector, rather than having to read in from the files.

```{r, echo=TRUE, eval=FALSE}
resume_jsons_for_all_resumes <- rep(NA,times=length(resume_ids))

for(i in 1:length(resume_ids))
{
resume_link <- paste0("https://resumes.indeed.com/resume/",resume_ids[i])
resume_read_as_text <- readLines(resume_link)
resume_read_as_text <- grep('JSON.parse',resume_read_as_text,value=TRUE)
x22_locations <- str_locate_all(resume_read_as_text,pattern='\\\\x22')[[1]]
resume_json <- substr(resume_read_as_text,x22_locations[1,1] - 1,x22_locations[nrow(x22_locations),2] + 1)
resume_json <- str_replace_all(resume_json,pattern='\\\\x22',replace='"')
resume_jsons_for_all_resumes[i] <- resume_json
if(i %% 50 == 0){print(paste0("Resume ",i," has been processed"))}
}

write.table(resume_jsons_for_all_resumes,
file="first_650_san_francisco_resume_json_files_concatenated.txt",
row.names=FALSE,col.names=FALSE,quote=FALSE)

save(resume_jsons_for_all_resumes,file="first_650_san_francisco_resume_json_files_concatenated.Rdata")
```

```{r, echo=FALSE, eval=TRUE}
load("first_650_san_francisco_resume_json_files_concatenated.Rdata")
```

## Step 6 - Process resume JSON files.

We create a series of lists to store each resume's job titles, job descriptions, and lists of skills.

We also create a vector to store each resume's summary section.

Last time, we found that some resumes had malformed JSONs, or were missing descriptions for some jobs.

Last time figured this out manually. Here we will try to automate these checks.

Use isValidJSON for check if malformed. Check number of job titles vs. descriptions to make sure all jobs have descriptions.

If both of these checks pass, add to the lists.

```{r}
valid_json <- rep(NA,times=length(resume_ids))
descriptions_for_every_job <- rep(NA,times=length(resume_ids))

job_titles <- vector("list",length=length(resume_ids))
job_descriptions <- job_titles
lists_of_skills <- job_titles
executive_summaries <- rep(NA,times=length(resume_ids))

for(i in 1:length(resume_ids))
{
validity_check <- isValidJSON(resume_jsons_for_all_resumes[i],asText = TRUE)
if(validity_check == FALSE){valid_json[i] <- FALSE;next}

resume_json <- fromJSON(resume_jsons_for_all_resumes[i],asText = TRUE)

job_titles_this_resume <- unlist(lapply(resume_json$resumeModel$workExperience,"[[","title"))
job_descriptions_this_resume <- unlist(lapply(resume_json$resumeModel$workExperience,"[[","description"))

descriptions_for_every_job_this_resume <- length(job_titles_this_resume) == length(job_descriptions_this_resume)
if(descriptions_for_every_job_this_resume == FALSE){descriptions_for_every_job[i] <- FALSE;next}

job_titles[[i]] <- job_titles_this_resume
job_descriptions[[i]] <- job_descriptions_this_resume

lists_of_skills[[i]] <- unlist(lapply(resume_json$resumeModel$skills,"[[","skill"))
executive_summaries[i] <- unlist(resume_json$resumeModel$summary)

valid_json[i] <- TRUE
descriptions_for_every_job[i] <- TRUE

if(i %% 50 == 0){print(paste0("Resume ",i," has been processed"))}
}

job_titles <- job_titles[valid_json == TRUE & descriptions_for_every_job == TRUE]
job_descriptions <- job_descriptions[valid_json == TRUE & descriptions_for_every_job == TRUE]
lists_of_skills <- lists_of_skills[valid_json == TRUE & descriptions_for_every_job == TRUE]
executive_summaries <- executive_summaries[valid_json == TRUE & descriptions_for_every_job == TRUE]

length(job_titles)
which(valid_json == FALSE)
which(descriptions_for_every_job == FALSE)
```

```{r, echo=FALSE, eval=FALSE}
save.image("after_for_loop_to_process_jsons_san_francisco.Rdata")
```

Looks like the number of resumes with malformed JSONs is substantially higher for this time than when we searched for New York, NY.

Nonetheless, 571 resumes is still quite a good sample size.

Let's proceed with making the data frames.

Use job_titles and job_descriptions list to make one big data frame.

In a given resume, there will often be multiple job title/description combinations. So, we repeat each resume index by the number of job titles there
 are.

For skills, there may also frequently be multiple skills per resume. So we follow a similar strategy there as well.

```{r, echo=TRUE, eval=TRUE}
job_titles_and_descriptions_across_resumes <- data.frame(Resume.num = rep(which(valid_json == TRUE & descriptions_for_every_job == TRUE),
							times=unlist(lapply(job_titles,function(x)length(x)))),
							Job.title = unlist(job_titles),
							Job.description = unlist(job_descriptions),
							stringsAsFactors=FALSE)
skills_per_resume <- data.frame(Resume.num = rep(which(valid_json == TRUE & descriptions_for_every_job == TRUE),
						times=unlist(lapply(lists_of_skills,function(x)length(x)))),
						Skill = unlist(lists_of_skills),
						stringsAsFactors=FALSE)
```

Save our work as we go along as good practice.

```{r}
save.image("resumes_processed_san_francisco.Rdata")
```

Display first and last rows of the data frames to get a sense of what they look like.

```{r}
head(job_titles_and_descriptions_across_resumes)
tail(job_titles_and_descriptions_across_resumes)
head(skills_per_resume)
tail(skills_per_resume)
```

