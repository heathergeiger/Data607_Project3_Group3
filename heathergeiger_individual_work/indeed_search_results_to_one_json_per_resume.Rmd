# Downloading Indeed.com Resumes Matching a Search as JSON files
# Heather Geiger - March 20, 2018

## Load libraries.

Load a bunch of libraries.

I'm actually not sure which of these I actually use, as some of this code is from Raj. Need to check that.

Currently I load all of the libraries Raj suggests, plus RJSONIO to process JSON files.

```{r}
library(stringr)    #For string operations
library(rvest)      #For screen scrapper
library(tokenizers) #
library(tidyverse)  #For Tidyverse
library(RCurl)      #For File Operations
library(dplyr)      #For Manipulating the data frames
library(DT)         #For Data table package
library(curl)

library(RJSONIO)
```

## Step 1 - Get search queries to paste into a browser.

We write a function to search for a given job title in a particular city and state.

```{r}
indeedUrlBuilder <- function(jobtitle, cityname, statecode){
startUrl <- "https://www.indeed.com/resumes?q="
jobtitle <- gsub(" ","+",jobtitle)
middle0Url <- "&l="
cityname <- gsub(" ","+",cityname)
middle1Url <- "%2C+"
endUrl <- "&searchField=jt"
searchUrl <- paste0(startUrl,jobtitle,middle0Url,cityname,middle1Url,statecode,endUrl)
return(searchUrl)
}
```

Run for data scientist in New York, NY.

```{r}
url_to_search <- indeedUrlBuilder("data scientist","New York","NY")
url_to_search
```

Paste this together with iterations of 50 from 50 to 950.
This gives pages 2-20 of the search results, so in total we can look at 1,000 resumes.

```{r}
for(i in seq(from=50,to=950,by=50))
{
print(paste0(url_to_search,"&start=",i))
}
```

## Step 2 - Paste search queries into a browser and use point-and-click to get commands to download JSON files using the API.

For each of these URLs, we use a browser plus point-and-click to get the curl command to download the JSON file for the page using the API.

While we do this, keep a text file called "curl_commands_first_1000_resumes.txt" open, where we will paste each command after copying.

I am eventually going to add screenshots here, but for now will just write text since can't get screenshots to work in Rmarkdown at the moment.

1. Open developer tools. In Google Chrome, you get this by going to "More Tools", then "Developer Tools", in the top right menu.
2. Go to tab "Network". 
3. Reload page. All network calls the page is making will now show.
4. Go to the one with type "fetch" whose name starts with "search".
5. Right click, click copy, then click "Copy as cURL".

## Step 3 - Run curl commands.

We run all 20 curl commands as system commands now.

Output the results of each to their own JSON file.

```{r, echo=TRUE,eval=FALSE}
dir.create("data_scientist_new_york_NY_search_results_json_files")

curl_commands <- readLines("curl_commands_first_1000_resumes.txt")

for(i in 1:length(curl_commands))
{
command <- curl_commands[i]
print(command)
system(paste0(command," > data_scientist_new_york_NY_search_results_json_files/resumes_pg",i,".json"))
}
```

```{r, echo=FALSE,eval=TRUE}
curl_commands <- readLines("curl_commands_first_1000_resumes.txt")

curl_commands
```

## Step 4 - Process the search result JSON files.

For each JSON file, read into R and use this to get the links to resumes.

```{r, echo=TRUE, eval=TRUE}
resume_ids <- rep(NA,times=1000)

for(i in 1:length(curl_commands))
{
json_search_results <- fromJSON(paste0("data_scientist_new_york_NY_search_results_json_files/resumes_pg",i,".json"))
j = i - 1
resume_ids[(j*50 + 1):(j*50 + 50)] <- unlist(lapply(json_search_results$results,"[[","accountKey"))
}

head(resume_ids)
tail(resume_ids)
```

## Step 5 - Download JSON files for resumes.

For a given value in resume_ids, we can get the corresponding resume link by pasting to "https://resumes.indeed.com/resume/".

Then, read in the file. Take the line in the HTML file that is being run through the JSON.parse command.

The JSON file will be bounded by curly brackets, then "\\x22" at the beginning. It will end with "\\x22", then curly brackets.

There will also be other "\\x22" occurences in between, so take the first and last one.

Convert "\\x22" to quote, and it should now be in a format where fromJSON will interpret it as a JSON.

Write out each JSON to a file.

```{r, echo=TRUE, eval=FALSE}
dir.create("data_scientist_new_york_NY_resume_json_files")

for(i in 1:length(resume_ids))
{
resume_link <- paste0("https://resumes.indeed.com/resume/",resume_ids[i])
resume_read_as_text <- readLines(resume_link)
resume_read_as_text <- grep('JSON.parse',resume_read_as_text,value=TRUE)
x22_locations <- str_locate_all(resume_read_as_text,pattern='\\\\x22')[[1]]
resume_json <- substr(resume_read_as_text,x22_locations[1,1] - 1,x22_locations[nrow(x22_locations),2] + 1)
resume_json <- str_replace_all(resume_json,pattern='\\\\x22',replace='"')
write.table(resume_json,file=paste0("data_scientist_new_york_NY_resume_json_files/resume_result_",i,".json"),
row.names=FALSE,col.names=FALSE,quote=FALSE)
if(i %% 50 == 0){print(paste0("Resume ",i," has been processed"))}
}
```

```{r, echo=FALSE,eval=TRUE}
for(i in 1:length(resume_ids))
{
if(i %% 50 == 0){print(paste0("Resume ",i," has been processed"))}
}
```

## Step 6 - Process resume JSON files.

We create a series of lists to store each resume's job titles, job descriptions, and lists of skills.

We also create a vector to store each resume's summary section.

Only thing is we run this for slightly less than the full 1,000 resumes.

After some debugging, it appears that 6/1000 resumes (indices 185, 200, 763, 786, 795, and 815) are malformed JSONs.

Another 8/1000 are missing descriptions for some jobs (indices 66,111,213,290,294,505,627,837).

Going to skip these, and then all the others should be fine.

I think an error rate of 14/1000 here (1.4%) is pretty reasonable, and we will still have plenty of data to work with.

```{r}
job_titles <- vector("list",length=1000)
job_descriptions <- job_titles
lists_of_skills <- job_titles
executive_summaries <- rep(NA,times=1000)

for(i in setdiff(1:1000,c(185,200,763,786,795,815,66,111,213,290,294,505,627,837)))
{
resume_json <- fromJSON(paste0("data_scientist_new_york_NY_resume_json_files/resume_result_",i,".json"))
job_titles[[i]] <- unlist(lapply(resume_json$resumeModel$workExperience,"[[","title"))
job_descriptions[[i]] <- unlist(lapply(resume_json$resumeModel$workExperience,"[[","description"))
lists_of_skills[[i]] <- unlist(lapply(resume_json$resumeModel$skills,"[[","skill"))
executive_summaries[i] <- unlist(resume_json$resumeModel$summary)
if(i %% 50 == 0){print(paste0("Resume ",i," has been processed"))}
}

job_titles <- job_titles[setdiff(1:1000,c(185,200,763,786,795,815,66,111,213,290,294,505,627,837))]
job_descriptions <- job_descriptions[setdiff(1:1000,c(185,200,763,786,795,815,66,111,213,290,294,505,627,837))]
lists_of_skills <- lists_of_skills[setdiff(1:1000,c(185,200,763,786,795,815,66,111,213,290,294,505,627,837))]

executive_summaries <- executive_summaries[setdiff(1:1000,c(185,200,763,786,795,815,66,111,213,290,294,505,627,837))]
```

Use job_titles and job_descriptions list to make one big data frame.

In a given resume, there will often be multiple job title/description combinations. So, we repeat each resume index by the number of job titles there are.

For skills, there may also frequently be multiple skills per resume. So we follow a similar strategy there as well.

```{r, echo=TRUE, eval=TRUE}
job_titles_and_descriptions_across_resumes <- data.frame(Resume.num = rep(setdiff(1:1000,c(185,200,763,786,795,815,66,111,213,290,294,505,627,837)),
							times=unlist(lapply(job_titles,function(x)length(x)))),
						Job.title = unlist(job_titles),
						Job.description = unlist(job_descriptions),
						stringsAsFactors=FALSE)
skills_per_resume <- data.frame(Resume.num = rep(setdiff(1:1000,c(185,200,763,786,795,815,66,111,213,290,294,505,627,837)),
				times=unlist(lapply(lists_of_skills,function(x)length(x)))),
				Skill = unlist(lists_of_skills),
				stringsAsFactors=FALSE)
```

Save our work as we go along as good practice.

```{r}
save.image("resumes_processed.Rdata")
```

Display first and last rows of the data frames to get a sense of what they look like.

```{r}
head(job_titles_and_descriptions_across_resumes)
tail(job_titles_and_descriptions_across_resumes)
head(skills_per_resume)
tail(skills_per_resume)
```

## Final step - clean up after make the data frames from the JSON files.

Just from looking at the head and tail, we can see we will need to do some clean-up here.

For one, we need to change the special encoding "\\u002F" to a "/".

We also see that sometimes the skills field was formatted so that each skill was an item in a vector, while sometimes it is free text, with a one-item free text character vector that might actually describe many skills.

Let's look at the first few skills with a comma in them.

```{r}
skills_with_comma <- skills_per_resume[grep(',',skills_per_resume[,2]),"Skill"]
head(skills_with_comma)
skills_with_comma <- skills_with_comma[skills_with_comma != "EXTRACT, TRANSFORM, AND LOAD"]
head(skills_with_comma,n=20)
```

Looks like we can't use the presence or absence of a comma to determine which is which without context. 

A Google reveals that "EXTRACT, TRANSFORM, AND LOAD" is actually one skill (https://en.wikipedia.org/wiki/Extract,_transform,_load).

If we are just run simple string processing for keywords on this column, I suppose it does not matter too much. But something to keep in mind.


